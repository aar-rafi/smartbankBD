{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2497231,"sourceType":"datasetVersion","datasetId":1512017},{"sourceId":669432,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":506919,"modelId":521662}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\nimport seaborn as sns\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:56:51.482885Z","iopub.execute_input":"2025-12-02T04:56:51.483729Z","iopub.status.idle":"2025-12-02T04:56:59.735428Z","shell.execute_reply.started":"2025-12-02T04:56:51.483674Z","shell.execute_reply":"2025-12-02T04:56:59.734734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Configuration ---\nCONFIG = {\n    \"dataset_path\": \"/kaggle/input/cedardataset/signatures\",  # Adjust if needed\n    \"model_path\": \"/kaggle/input/siamese-transformer/pytorch/default/1/best_siamese_transformer.pth\",  # Update this path\n    \"img_size\": (224, 224),\n    \"batch_size\": 32,\n    \"embedding_dim\": 128,\n    \"transformer_heads\": 4,\n    \"transformer_layers\": 2,\n    \"dropout\": 0.1,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"seed\": 42,\n    \"threshold\": 0.5  # Distance threshold for classification\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:56:59.736507Z","iopub.execute_input":"2025-12-02T04:56:59.736853Z","iopub.status.idle":"2025-12-02T04:56:59.795496Z","shell.execute_reply.started":"2025-12-02T04:56:59.736834Z","shell.execute_reply":"2025-12-02T04:56:59.794662Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Seeding for Reproducibility ---\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(CONFIG['seed'])\nprint(f\"Device: {CONFIG['device']}\")\nprint(f\"Dataset Path: {CONFIG['dataset_path']}\")\nprint(f\"Model Path: {CONFIG['model_path']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:56:59.796574Z","iopub.execute_input":"2025-12-02T04:56:59.796919Z","iopub.status.idle":"2025-12-02T04:56:59.992374Z","shell.execute_reply.started":"2025-12-02T04:56:59.796891Z","shell.execute_reply":"2025-12-02T04:56:59.991568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Dataset Class (Same as training) ---\nclass SignatureDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.genuine_path = os.path.join(root_dir, 'full_org')\n        self.forged_path = os.path.join(root_dir, 'full_forg')\n        \n        # Group signatures by writer ID\n        self.writers = {}\n        all_genuine = sorted(os.listdir(self.genuine_path))\n        all_forged = sorted(os.listdir(self.forged_path))\n        \n        for f in all_genuine:\n            if not f.startswith(\"original_\") or not f.endswith(\".png\"):\n                continue\n            try:\n                writer_id = int(f.split('_')[1])\n            except (IndexError, ValueError):\n                continue\n            \n            if writer_id not in self.writers:\n                self.writers[writer_id] = {'genuine': [], 'forged': []}\n            self.writers[writer_id]['genuine'].append(os.path.join(self.genuine_path, f))\n            \n        for f in all_forged:\n            if not f.startswith(\"forgeries_\") or not f.endswith(\".png\"):\n                continue\n            try:\n                writer_id = int(f.split('_')[1])\n            except (IndexError, ValueError):\n                continue\n            if writer_id in self.writers:\n                self.writers[writer_id]['forged'].append(os.path.join(self.forged_path, f))\n        \n        # Split writers (Writer-Independent Split)\n        writer_ids = list(self.writers.keys())\n        train_ids, test_ids = train_test_split(writer_ids, test_size=0.2, random_state=CONFIG['seed'])\n        \n        if split == 'train':\n            self.active_writers = train_ids\n        else:\n            self.active_writers = test_ids\n            \n        self.pairs = self._generate_pairs()\n\n    def _generate_pairs(self):\n        pairs = []\n        for wid in self.active_writers:\n            gens = self.writers[wid]['genuine']\n            forgs = self.writers[wid]['forged']\n            \n            # Positive Pairs (Genuine-Genuine)\n            for i in range(len(gens)):\n                for j in range(i + 1, len(gens)):\n                    pairs.append([gens[i], gens[j], 0])  # 0 = Similar\n            \n            # Negative Pairs (Genuine-Forged)\n            for g in gens:\n                for f in forgs:\n                    pairs.append([g, f, 1])  # 1 = Dissimilar\n                    \n        return pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img1_path, img2_path, label = self.pairs[idx]\n        \n        img1 = Image.open(img1_path).convert(\"L\").convert(\"RGB\")\n        img2 = Image.open(img2_path).convert(\"L\").convert(\"RGB\")\n        \n        if self.transform:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n            \n        return img1, img2, torch.tensor(label, dtype=torch.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:56:59.993056Z","iopub.execute_input":"2025-12-02T04:56:59.993278Z","iopub.status.idle":"2025-12-02T04:57:00.005185Z","shell.execute_reply.started":"2025-12-02T04:56:59.993254Z","shell.execute_reply":"2025-12-02T04:57:00.004440Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Model Architecture (Same as training) ---\nclass SiameseTransformer(nn.Module):\n    def __init__(self):\n        super(SiameseTransformer, self).__init__()\n        \n        efficientnet = models.efficientnet_b0(pretrained=False)  # No need to download weights\n        self.backbone = efficientnet.features\n        \n        self.feature_dim = 1280 \n        self.seq_len = 7 * 7\n        \n        self.pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, self.feature_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.feature_dim, \n            nhead=CONFIG['transformer_heads'], \n            dim_feedforward=self.feature_dim * 2,\n            dropout=CONFIG['dropout'],\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=CONFIG['transformer_layers'])\n        \n        self.fc = nn.Sequential(\n            nn.Linear(self.feature_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, CONFIG['embedding_dim'])\n        )\n\n    def forward_one(self, x):\n        features = self.backbone(x)\n        features = features.view(features.size(0), self.feature_dim, -1)\n        features = features.permute(0, 2, 1)\n        features = features + self.pos_embedding\n        features = self.transformer(features)\n        embedding = torch.mean(features, dim=1)\n        embedding = self.fc(embedding)\n        return embedding\n\n    def forward(self, img1, img2):\n        out1 = self.forward_one(img1)\n        out2 = self.forward_one(img2)\n        return out1, out2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:57:00.006810Z","iopub.execute_input":"2025-12-02T04:57:00.007021Z","iopub.status.idle":"2025-12-02T04:57:00.022201Z","shell.execute_reply.started":"2025-12-02T04:57:00.007005Z","shell.execute_reply":"2025-12-02T04:57:00.021441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Transformations ---\ntest_transforms = transforms.Compose([\n    transforms.Resize(CONFIG['img_size']),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# --- Load Test Dataset ---\nprint(\"\\nLoading test dataset...\")\ntest_ds = SignatureDataset(CONFIG['dataset_path'], split='test', transform=test_transforms)\ntest_loader = DataLoader(test_ds, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=2)\nprint(f\"Test Pairs: {len(test_ds)}\")\n\n# --- Load Model ---\nprint(\"\\nLoading trained model...\")\nmodel = SiameseTransformer().to(CONFIG['device'])\nmodel.load_state_dict(torch.load(CONFIG['model_path'], map_location=CONFIG['device']))\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n# --- Evaluation Function ---\ndef evaluate_model(model, dataloader, threshold=0.5):\n    \"\"\"\n    Evaluate the model and return predictions, labels, and distances\n    \"\"\"\n    all_distances = []\n    all_labels = []\n    all_predictions = []\n    \n    print(\"\\nEvaluating model...\")\n    with torch.no_grad():\n        for img1, img2, labels in tqdm(dataloader, desc=\"Testing\"):\n            img1, img2, labels = img1.to(CONFIG['device']), img2.to(CONFIG['device']), labels.to(CONFIG['device'])\n            \n            # Get embeddings\n            emb1, emb2 = model(img1, img2)\n            \n            # Calculate distances\n            distances = F.pairwise_distance(emb1, emb2)\n            \n            # Predictions based on threshold\n            # Distance < threshold â†’ Similar (0), Distance >= threshold â†’ Dissimilar (1)\n            predictions = (distances >= threshold).float()\n            \n            all_distances.extend(distances.cpu().numpy())\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predictions.cpu().numpy())\n    \n    return np.array(all_predictions), np.array(all_labels), np.array(all_distances)\n\n# --- Run Evaluation ---\npredictions, true_labels, distances = evaluate_model(model, test_loader, threshold=CONFIG['threshold'])\n\n# --- Calculate Metrics ---\naccuracy = accuracy_score(true_labels, predictions)\nprecision = precision_score(true_labels, predictions)\nrecall = recall_score(true_labels, predictions)\nf1 = f1_score(true_labels, predictions)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"MODEL EVALUATION RESULTS\")\nprint(\"=\"*50)\nprint(f\"Threshold: {CONFIG['threshold']}\")\nprint(f\"Total Test Pairs: {len(test_ds)}\")\nprint(f\"\\nAccuracy:  {accuracy*100:.2f}%\")\nprint(f\"Precision: {precision*100:.2f}%\")\nprint(f\"Recall:    {recall*100:.2f}%\")\nprint(f\"F1-Score:  {f1*100:.2f}%\")\n\n# --- Confusion Matrix ---\ncm = confusion_matrix(true_labels, predictions)\nprint(f\"\\nConfusion Matrix:\")\nprint(f\"                 Predicted\")\nprint(f\"                 Similar  Dissimilar\")\nprint(f\"Actual Similar   {cm[0,0]:>6}   {cm[0,1]:>6}\")\nprint(f\"Actual Dissimilar {cm[1,0]:>6}   {cm[1,1]:>6}\")\n\n# --- Visualizations ---\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# 1. Confusion Matrix Heatmap\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0],\n            xticklabels=['Similar', 'Dissimilar'],\n            yticklabels=['Similar', 'Dissimilar'])\naxes[0,0].set_title('Confusion Matrix')\naxes[0,0].set_ylabel('True Label')\naxes[0,0].set_xlabel('Predicted Label')\n\n# 2. Distance Distribution\naxes[0,1].hist(distances[true_labels==0], bins=50, alpha=0.6, label='Genuine Pairs', color='green')\naxes[0,1].hist(distances[true_labels==1], bins=50, alpha=0.6, label='Forged Pairs', color='red')\naxes[0,1].axvline(CONFIG['threshold'], color='black', linestyle='--', linewidth=2, label='Threshold')\naxes[0,1].set_xlabel('Distance')\naxes[0,1].set_ylabel('Frequency')\naxes[0,1].set_title('Distance Distribution')\naxes[0,1].legend()\n\n# 3. ROC Curve\nfpr, tpr, thresholds = roc_curve(true_labels, distances)\nroc_auc = auc(fpr, tpr)\naxes[1,0].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set_xlim([0.0, 1.0])\naxes[1,0].set_ylim([0.0, 1.05])\naxes[1,0].set_xlabel('False Positive Rate')\naxes[1,0].set_ylabel('True Positive Rate')\naxes[1,0].set_title('ROC Curve')\naxes[1,0].legend(loc=\"lower right\")\n\n# 4. Metrics Bar Chart\nmetrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\nvalues = [accuracy*100, precision*100, recall*100, f1*100]\ncolors = ['#3498db', '#2ecc71', '#e74c3c', '#f39c12']\naxes[1,1].bar(metrics, values, color=colors)\naxes[1,1].set_ylim([0, 100])\naxes[1,1].set_ylabel('Score (%)')\naxes[1,1].set_title('Performance Metrics')\nfor i, v in enumerate(values):\n    axes[1,1].text(i, v + 2, f'{v:.2f}%', ha='center', fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('evaluation_results.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# --- Find Optimal Threshold ---\nprint(\"\\n\" + \"=\"*50)\nprint(\"THRESHOLD OPTIMIZATION\")\nprint(\"=\"*50)\n\nthresholds_to_test = np.linspace(0.1, 2.0, 20)\naccuracies = []\n\nfor thresh in thresholds_to_test:\n    preds = (distances >= thresh).astype(float)\n    acc = accuracy_score(true_labels, preds)\n    accuracies.append(acc)\n\noptimal_idx = np.argmax(accuracies)\noptimal_threshold = thresholds_to_test[optimal_idx]\noptimal_accuracy = accuracies[optimal_idx]\n\nprint(f\"Optimal Threshold: {optimal_threshold:.4f}\")\nprint(f\"Optimal Accuracy: {optimal_accuracy*100:.2f}%\")\n\n# Plot threshold vs accuracy\nplt.figure(figsize=(10, 6))\nplt.plot(thresholds_to_test, np.array(accuracies)*100, marker='o', linewidth=2)\nplt.axvline(optimal_threshold, color='red', linestyle='--', linewidth=2, label=f'Optimal: {optimal_threshold:.4f}')\nplt.axvline(CONFIG['threshold'], color='green', linestyle='--', linewidth=2, label=f'Current: {CONFIG[\"threshold\"]:.4f}')\nplt.xlabel('Threshold', fontsize=12)\nplt.ylabel('Accuracy (%)', fontsize=12)\nplt.title('Threshold vs Accuracy', fontsize=14, fontweight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.savefig('threshold_optimization.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# --- Sample Predictions Visualization ---\nprint(\"\\n\" + \"=\"*50)\nprint(\"SAMPLE PREDICTIONS\")\nprint(\"=\"*50)\n\ndef visualize_predictions(dataset, model, num_samples=6, threshold=0.5):\n    \"\"\"Visualize random predictions from the test set\"\"\"\n    inv_normalize = transforms.Normalize(\n        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n        std=[1/0.229, 1/0.224, 1/0.225]\n    )\n    \n    fig, axes = plt.subplots(num_samples//2, 4, figsize=(16, num_samples*2))\n    \n    for i in range(num_samples//2):\n        # Get random sample\n        idx = random.randint(0, len(dataset)-1)\n        img1, img2, label = dataset[idx]\n        \n        # Get prediction\n        model.eval()\n        with torch.no_grad():\n            emb1, emb2 = model(img1.unsqueeze(0).to(CONFIG['device']), \n                              img2.unsqueeze(0).to(CONFIG['device']))\n            dist = F.pairwise_distance(emb1, emb2).item()\n        \n        pred_label = \"GENUINE\" if dist < threshold else \"FORGED\"\n        actual_label = \"GENUINE\" if label.item() == 0 else \"FORGED\"\n        is_correct = pred_label == actual_label\n        \n        # Denormalize images\n        img1_display = inv_normalize(img1).permute(1, 2, 0).cpu().numpy()\n        img2_display = inv_normalize(img2).permute(1, 2, 0).cpu().numpy()\n        \n        # Plot\n        axes[i, 0].imshow(np.clip(img1_display, 0, 1))\n        axes[i, 0].set_title(\"Reference\", fontsize=10)\n        axes[i, 0].axis('off')\n        \n        axes[i, 1].imshow(np.clip(img2_display, 0, 1))\n        axes[i, 1].set_title(\"Test\", fontsize=10)\n        axes[i, 1].axis('off')\n        \n        # Info\n        color = 'green' if is_correct else 'red'\n        info_text = f\"True: {actual_label}\\nPred: {pred_label}\\nDist: {dist:.4f}\\n{'âœ“ CORRECT' if is_correct else 'âœ— WRONG'}\"\n        axes[i, 2].text(0.5, 0.5, info_text, ha='center', va='center', \n                       fontsize=11, fontweight='bold', color=color,\n                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n        axes[i, 2].axis('off')\n        \n        # Distance bar\n        axes[i, 3].barh(['Distance'], [dist], color='blue' if dist < threshold else 'red')\n        axes[i, 3].axvline(threshold, color='black', linestyle='--', linewidth=2)\n        axes[i, 3].set_xlim([0, max(2, dist+0.5)])\n        axes[i, 3].set_xlabel('Distance', fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig('sample_predictions.png', dpi=300, bbox_inches='tight')\n    plt.show()\n\nvisualize_predictions(test_ds, model, num_samples=6, threshold=CONFIG['threshold'])\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"Evaluation complete! Results saved as PNG files.\")\nprint(\"=\"*50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T04:57:00.023091Z","iopub.execute_input":"2025-12-02T04:57:00.023324Z","iopub.status.idle":"2025-12-02T04:58:14.079028Z","shell.execute_reply.started":"2025-12-02T04:57:00.023308Z","shell.execute_reply":"2025-12-02T04:58:14.078390Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport cv2\nfrom tqdm import tqdm\n\n# --- Configuration ---\nCONFIG = {\n    \"dataset_path\": \"/kaggle/input/cedardataset/signatures\",\n    \"model_path\": \"/kaggle/input/siamese-transformer/pytorch/default/1/best_siamese_transformer.pth\",\n    \"img_size\": (224, 224),\n    \"batch_size\": 32,\n    \"embedding_dim\": 128,\n    \"transformer_heads\": 4,\n    \"transformer_layers\": 2,\n    \"dropout\": 0.1,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n    \"seed\": 42,\n    \"threshold\": 0.5\n}\n\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_everything(CONFIG['seed'])\n\n# --- Dataset Class ---\nclass SignatureDataset(Dataset):\n    def __init__(self, root_dir, split='train', transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.genuine_path = os.path.join(root_dir, 'full_org')\n        self.forged_path = os.path.join(root_dir, 'full_forg')\n        \n        self.writers = {}\n        all_genuine = sorted(os.listdir(self.genuine_path))\n        all_forged = sorted(os.listdir(self.forged_path))\n        \n        for f in all_genuine:\n            if not f.startswith(\"original_\") or not f.endswith(\".png\"):\n                continue\n            try:\n                writer_id = int(f.split('_')[1])\n            except (IndexError, ValueError):\n                continue\n            \n            if writer_id not in self.writers:\n                self.writers[writer_id] = {'genuine': [], 'forged': []}\n            self.writers[writer_id]['genuine'].append(os.path.join(self.genuine_path, f))\n            \n        for f in all_forged:\n            if not f.startswith(\"forgeries_\") or not f.endswith(\".png\"):\n                continue\n            try:\n                writer_id = int(f.split('_')[1])\n            except (IndexError, ValueError):\n                continue\n            if writer_id in self.writers:\n                self.writers[writer_id]['forged'].append(os.path.join(self.forged_path, f))\n        \n        writer_ids = list(self.writers.keys())\n        train_ids, test_ids = train_test_split(writer_ids, test_size=0.2, random_state=CONFIG['seed'])\n        \n        if split == 'train':\n            self.active_writers = train_ids\n        else:\n            self.active_writers = test_ids\n            \n        self.pairs = self._generate_pairs()\n\n    def _generate_pairs(self):\n        pairs = []\n        for wid in self.active_writers:\n            gens = self.writers[wid]['genuine']\n            forgs = self.writers[wid]['forged']\n            \n            for i in range(len(gens)):\n                for j in range(i + 1, len(gens)):\n                    pairs.append([gens[i], gens[j], 0])\n            \n            for g in gens:\n                for f in forgs:\n                    pairs.append([g, f, 1])\n                    \n        return pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        img1_path, img2_path, label = self.pairs[idx]\n        \n        img1 = Image.open(img1_path).convert(\"L\").convert(\"RGB\")\n        img2 = Image.open(img2_path).convert(\"L\").convert(\"RGB\")\n        \n        if self.transform:\n            img1 = self.transform(img1)\n            img2 = self.transform(img2)\n            \n        return img1, img2, torch.tensor(label, dtype=torch.float32), img1_path, img2_path\n\n# --- Modified Model with Patch Embedding Extraction ---\nclass SiameseTransformerExplainable(nn.Module):\n    def __init__(self):\n        super(SiameseTransformerExplainable, self).__init__()\n        \n        efficientnet = models.efficientnet_b0(pretrained=False)\n        self.backbone = efficientnet.features\n        \n        self.feature_dim = 1280 \n        self.seq_len = 7 * 7  # 49 patches\n        self.patch_grid = (7, 7)  # Spatial dimensions\n        \n        self.pos_embedding = nn.Parameter(torch.randn(1, self.seq_len, self.feature_dim))\n        \n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=self.feature_dim, \n            nhead=CONFIG['transformer_heads'], \n            dim_feedforward=self.feature_dim * 2,\n            dropout=CONFIG['dropout'],\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=CONFIG['transformer_layers'])\n        \n        self.fc = nn.Sequential(\n            nn.Linear(self.feature_dim, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, CONFIG['embedding_dim'])\n        )\n\n    def forward_one(self, x, return_patches=False):\n        # Extract features -> [Batch, 1280, 7, 7]\n        features = self.backbone(x)\n        \n        # Flatten spatial dims to sequence -> [Batch, 1280, 49]\n        features = features.view(features.size(0), self.feature_dim, -1)\n        \n        # Transpose for Transformer -> [Batch, 49, 1280]\n        features = features.permute(0, 2, 1)\n        \n        # Add Positional Encoding\n        features = features + self.pos_embedding\n        \n        # Pass through Transformer\n        patch_embeddings = self.transformer(features)  # [Batch, 49, 1280]\n        \n        if return_patches:\n            return patch_embeddings\n        \n        # Global Average Pooling\n        embedding = torch.mean(patch_embeddings, dim=1)\n        \n        # Final Projection\n        embedding = self.fc(embedding)\n        return embedding\n\n    def forward(self, img1, img2, return_patches=False):\n        if return_patches:\n            patches1 = self.forward_one(img1, return_patches=True)\n            patches2 = self.forward_one(img2, return_patches=True)\n            return patches1, patches2\n        else:\n            out1 = self.forward_one(img1)\n            out2 = self.forward_one(img2)\n            return out1, out2\n\n# --- Heatmap Generation Functions ---\ndef compute_patch_difference_heatmap(model, img1, img2, method='cosine'):\n    \"\"\"\n    Compute patch-level differences between two signatures\n    \n    Args:\n        model: The Siamese model\n        img1, img2: Input tensors [1, 3, 224, 224]\n        method: 'cosine' or 'euclidean'\n    \n    Returns:\n        heatmap: 2D array showing differences\n    \"\"\"\n    model.eval()\n    with torch.no_grad():\n        # Get patch embeddings [1, 49, 1280]\n        patches1, patches2 = model(img1, img2, return_patches=True)\n        \n        # Remove batch dimension [49, 1280]\n        patches1 = patches1.squeeze(0)\n        patches2 = patches2.squeeze(0)\n        \n        if method == 'cosine':\n            # Compute cosine similarity for each patch\n            # Normalize the embeddings\n            patches1_norm = F.normalize(patches1, p=2, dim=1)\n            patches2_norm = F.normalize(patches2, p=2, dim=1)\n            \n            # Cosine similarity (element-wise)\n            similarity = (patches1_norm * patches2_norm).sum(dim=1)  # [49]\n            \n            # Convert to difference (1 - similarity)\n            difference = 1 - similarity  # [49]\n            \n        elif method == 'euclidean':\n            # Euclidean distance\n            difference = torch.norm(patches1 - patches2, dim=1)  # [49]\n            # Normalize to [0, 1]\n            difference = difference / difference.max()\n        \n        # Reshape to 2D grid [7, 7]\n        heatmap = difference.view(model.patch_grid).cpu().numpy()\n        \n    return heatmap\n\ndef overlay_heatmap(original_img, heatmap, alpha=0.6, colormap=cv2.COLORMAP_JET):\n    \"\"\"\n    Overlay heatmap on original image\n    \n    Args:\n        original_img: PIL Image or numpy array\n        heatmap: 2D numpy array [7, 7]\n        alpha: Transparency of heatmap\n        colormap: OpenCV colormap\n    \n    Returns:\n        overlayed image as numpy array\n    \"\"\"\n    # Convert PIL to numpy if needed\n    if isinstance(original_img, Image.Image):\n        original_img = np.array(original_img)\n    \n    # Ensure original is RGB\n    if original_img.shape[-1] != 3:\n        original_img = cv2.cvtColor(original_img, cv2.COLOR_GRAY2RGB)\n    \n    # Get original size\n    h, w = original_img.shape[:2]\n    \n    # Normalize heatmap to [0, 255]\n    heatmap_normalized = ((heatmap - heatmap.min()) / (heatmap.max() - heatmap.min() + 1e-8) * 255).astype(np.uint8)\n    \n    # Resize heatmap to match original image size using bilinear interpolation\n    heatmap_resized = cv2.resize(heatmap_normalized, (w, h), interpolation=cv2.INTER_LINEAR)\n    \n    # Apply colormap\n    heatmap_colored = cv2.applyColorMap(heatmap_resized, colormap)\n    heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)\n    \n    # Overlay\n    overlayed = cv2.addWeighted(original_img, 1-alpha, heatmap_colored, alpha, 0)\n    \n    return overlayed, heatmap_resized\n\ndef create_comprehensive_visualization(model, img1_tensor, img2_tensor, img1_pil, img2_pil, \n                                       label, threshold=0.5, method='cosine'):\n    \"\"\"\n    Create a comprehensive visualization with heatmaps\n    \"\"\"\n    model.eval()\n    \n    # Get final embeddings and distance\n    with torch.no_grad():\n        emb1, emb2 = model(img1_tensor, img2_tensor)\n        distance = F.pairwise_distance(emb1, emb2).item()\n    \n    # Get heatmap\n    heatmap = compute_patch_difference_heatmap(model, img1_tensor, img2_tensor, method=method)\n    \n    # Create overlays\n    overlay1, heatmap_resized = overlay_heatmap(img1_pil, heatmap, alpha=0.5)\n    overlay2, _ = overlay_heatmap(img2_pil, heatmap, alpha=0.5)\n    \n    # Determine prediction\n    is_genuine = distance < threshold\n    pred_label = \"GENUINE\" if is_genuine else \"FORGED\"\n    true_label = \"GENUINE\" if label == 0 else \"FORGED\"\n    is_correct = pred_label == true_label\n    \n    # Create visualization\n    fig = plt.figure(figsize=(18, 10))\n    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n    \n    # Row 1: Original images\n    ax1 = fig.add_subplot(gs[0, 0])\n    ax1.imshow(img1_pil)\n    ax1.set_title(\"Reference Signature\", fontsize=12, fontweight='bold')\n    ax1.axis('off')\n    \n    ax2 = fig.add_subplot(gs[0, 1])\n    ax2.imshow(img2_pil)\n    ax2.set_title(\"Test Signature\", fontsize=12, fontweight='bold')\n    ax2.axis('off')\n    \n    # Row 1: Difference heatmap (raw)\n    ax3 = fig.add_subplot(gs[0, 2])\n    im = ax3.imshow(heatmap, cmap='jet', interpolation='nearest')\n    ax3.set_title(\"Patch Difference Map\\n(7Ã—7 patches)\", fontsize=12, fontweight='bold')\n    ax3.axis('off')\n    plt.colorbar(im, ax=ax3, fraction=0.046, pad=0.04)\n    \n    # Row 1: Prediction info\n    ax4 = fig.add_subplot(gs[0, 3])\n    ax4.axis('off')\n    \n    color = 'green' if is_correct else 'red'\n    status = 'âœ“ CORRECT' if is_correct else 'âœ— INCORRECT'\n    \n    info_text = f\"\"\"\n    PREDICTION RESULTS\n    {'='*30}\n    \n    Ground Truth: {true_label}\n    Prediction: {pred_label}\n    \n    Distance: {distance:.4f}\n    Threshold: {threshold:.4f}\n    \n    Status: {status}\n    \n    Method: {method.upper()}\n    \"\"\"\n    \n    ax4.text(0.1, 0.5, info_text, fontsize=11, family='monospace',\n             verticalalignment='center',\n             bbox=dict(boxstyle='round', facecolor=color, alpha=0.2))\n    \n    # Row 2: Overlayed heatmaps\n    ax5 = fig.add_subplot(gs[1, 0])\n    ax5.imshow(overlay1)\n    ax5.set_title(\"Reference + Heatmap\", fontsize=12, fontweight='bold')\n    ax5.axis('off')\n    \n    ax6 = fig.add_subplot(gs[1, 1])\n    ax6.imshow(overlay2)\n    ax6.set_title(\"Test + Heatmap\", fontsize=12, fontweight='bold')\n    ax6.axis('off')\n    \n    # Row 2: High-resolution heatmap\n    ax7 = fig.add_subplot(gs[1, 2])\n    im2 = ax7.imshow(heatmap_resized, cmap='jet', interpolation='bilinear')\n    ax7.set_title(\"Upsampled Heatmap\\n(224Ã—224)\", fontsize=12, fontweight='bold')\n    ax7.axis('off')\n    plt.colorbar(im2, ax=ax7, fraction=0.046, pad=0.04)\n    \n    # Row 2: Interpretation guide\n    ax8 = fig.add_subplot(gs[1, 3])\n    ax8.axis('off')\n    \n    guide_text = \"\"\"\n    HEATMAP INTERPRETATION\n    {'='*30}\n    \n    ðŸ”µ BLUE regions:\n       Low difference\n       Patches match well\n       Similar strokes\n    \n    ðŸŸ¢ GREEN regions:\n       Moderate difference\n       Some variation\n    \n    ðŸŸ¡ YELLOW regions:\n       High difference\n       Significant variation\n    \n    ðŸ”´ RED regions:\n       Very high difference\n       Strong mismatch\n       Different strokes/angles\n    \"\"\"\n    \n    ax8.text(0.1, 0.5, guide_text, fontsize=10, family='monospace',\n             verticalalignment='center',\n             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.3))\n    \n    # Row 3: Patch-level analysis\n    ax9 = fig.add_subplot(gs[2, :2])\n    \n    # Flatten heatmap and find top differences\n    flat_heatmap = heatmap.flatten()\n    top_k = 5\n    top_indices = np.argsort(flat_heatmap)[-top_k:][::-1]\n    \n    patch_positions = []\n    patch_values = []\n    for idx in top_indices:\n        row = idx // 7\n        col = idx % 7\n        patch_positions.append(f\"Patch ({row},{col})\")\n        patch_values.append(flat_heatmap[idx])\n    \n    colors_bar = ['red' if v > 0.7 else 'orange' if v > 0.5 else 'yellow' for v in patch_values]\n    ax9.barh(patch_positions, patch_values, color=colors_bar)\n    ax9.set_xlabel('Difference Score', fontsize=11)\n    ax9.set_title(f'Top {top_k} Most Different Patches', fontsize=12, fontweight='bold')\n    ax9.set_xlim([0, 1])\n    \n    # Row 3: Statistics\n    ax10 = fig.add_subplot(gs[2, 2:])\n    ax10.axis('off')\n    \n    stats_text = f\"\"\"\n    STATISTICAL ANALYSIS\n    {'='*35}\n    \n    Heatmap Statistics:\n    â€¢ Mean Difference:    {heatmap.mean():.4f}\n    â€¢ Max Difference:     {heatmap.max():.4f}\n    â€¢ Min Difference:     {heatmap.min():.4f}\n    â€¢ Std Deviation:      {heatmap.std():.4f}\n    \n    Patch Analysis:\n    â€¢ Total Patches:      {heatmap.size}\n    â€¢ High Diff (>0.7):   {(heatmap > 0.7).sum()} patches\n    â€¢ Medium Diff (0.5-0.7): {((heatmap > 0.5) & (heatmap <= 0.7)).sum()} patches\n    â€¢ Low Diff (<0.5):    {(heatmap <= 0.5).sum()} patches\n    \n    Classification:\n    â€¢ Distance:           {distance:.4f}\n    â€¢ Margin from threshold: {abs(distance - threshold):.4f}\n    \"\"\"\n    \n    ax10.text(0.1, 0.5, stats_text, fontsize=10, family='monospace',\n             verticalalignment='center',\n             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.2))\n    \n    # Main title\n    title_color = 'green' if is_correct else 'red'\n    fig.suptitle(f'Explainable Signature Verification - {pred_label} ({status})', \n                 fontsize=16, fontweight='bold', color=title_color, y=0.98)\n    \n    return fig, heatmap\n\n# --- Load Model and Dataset ---\nprint(\"Loading model and dataset...\")\n\ntest_transforms = transforms.Compose([\n    transforms.Resize(CONFIG['img_size']),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_ds = SignatureDataset(CONFIG['dataset_path'], split='test', transform=test_transforms)\nprint(f\"Test Pairs: {len(test_ds)}\")\n\nmodel = SiameseTransformerExplainable().to(CONFIG['device'])\nmodel.load_state_dict(torch.load(CONFIG['model_path'], map_location=CONFIG['device']))\nmodel.eval()\nprint(\"Model loaded successfully!\")\n\n# --- Generate Visualizations ---\nprint(\"\\n\" + \"=\"*60)\nprint(\"GENERATING EXPLAINABLE HEATMAP VISUALIZATIONS\")\nprint(\"=\"*60)\n\n# Inverse normalization for display\ninv_normalize = transforms.Normalize(\n    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n    std=[1/0.229, 1/0.224, 1/0.225]\n)\n\nnum_samples = 4  # Number of sample pairs to visualize\n\nfor i in range(num_samples):\n    # Get a random sample\n    idx = random.randint(0, len(test_ds)-1)\n    img1_tensor, img2_tensor, label, path1, path2 = test_ds[idx]\n    \n    # Load original PIL images for overlay\n    img1_pil = Image.open(path1).convert(\"RGB\")\n    img2_pil = Image.open(path2).convert(\"RGB\")\n    \n    # Resize to match model input\n    img1_pil = img1_pil.resize(CONFIG['img_size'])\n    img2_pil = img2_pil.resize(CONFIG['img_size'])\n    \n    # Add batch dimension\n    img1_batch = img1_tensor.unsqueeze(0).to(CONFIG['device'])\n    img2_batch = img2_tensor.unsqueeze(0).to(CONFIG['device'])\n    \n    # Create visualization\n    print(f\"\\nGenerating visualization {i+1}/{num_samples}...\")\n    fig, heatmap = create_comprehensive_visualization(\n        model, img1_batch, img2_batch, img1_pil, img2_pil, \n        label.item(), threshold=CONFIG['threshold'], method='cosine'\n    )\n    \n    plt.savefig(f'explainable_signature_{i+1}.png', dpi=300, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"Saved: explainable_signature_{i+1}.png\")\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"VISUALIZATION COMPLETE!\")\nprint(\"=\"*60)\nprint(\"\\nKey Insights from Heatmap Analysis:\")\nprint(\"â€¢ RED regions indicate strong mismatches (different strokes/angles)\")\nprint(\"â€¢ BLUE regions show good matches (similar strokes)\")\nprint(\"â€¢ The heatmap helps identify WHICH parts of signatures differ\")\nprint(\"â€¢ This provides explainability beyond just a distance number\")\nprint(\"â€¢ Useful for forensic analysis and understanding model decisions\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-02T05:03:47.487894Z","iopub.execute_input":"2025-12-02T05:03:47.488423Z","iopub.status.idle":"2025-12-02T05:03:59.081271Z","shell.execute_reply.started":"2025-12-02T05:03:47.488398Z","shell.execute_reply":"2025-12-02T05:03:59.080407Z"}},"outputs":[],"execution_count":null}]}